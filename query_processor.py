import logging
from typing import List
from langchain_community.llms import Ollama
from langchain.vectorstores import Chroma
from langchain.schema import Document

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

def process_query(db: Chroma, query: str = "Can you explain me about the attention in transformers") -> str:
    """
    Process a query using a vector store and Ollama LLM.

    Args:
        db (Chroma): Chroma vector store containing embedded documents.
        query (str): Query to process. Defaults to a sample question about transformers.

    Returns:
        str: Response generated by the LLM based on the context.

    Raises:
        Exception: If similarity search or LLM generation fails.
    """
    try:
        logger.info(f"Processing query: {query}")
        llm: Ollama = Ollama(model="llama3.1")
        
        logger.info("Performing similarity search")
        num_documents: int = len(db)
        result: List[Document] = db.similarity_search(query, num_documents)
        context: str = result[0].page_content  # Using top result; could combine multiple
        
        logger.info("Generating response with Ollama")
        prompt: str = f"Given the following information, please answer the question: \n\nContext: {context}\n\nQuestion: {query}"
        response: str = llm(prompt)
        logger.info("Response generated successfully")
        return response
    except Exception as e:
        logger.error(f"Error processing query: {e}")
        raise

if __name__ == "__main__":
    from pdf_loader import load_and_split_pdf
    from vector_store import create_vector_store
    documents: List[Document] = load_and_split_pdf()
    db: Chroma = create_vector_store(documents)
    response: str = process_query(db)
    logger.info(f"Response: {response}")